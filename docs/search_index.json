[["index.html", "Guidelines on Development of National Soil Legacy Data Repository Licence", " Guidelines on Development of National Soil Legacy Data Repository Rodriguez Lado, L., Angelini, M.E, Luotto, I., Yigini 2023-11-23 Licence The NLSDR Guideline Manual is made available under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 IGO licence CC BY-NC-SA 3.0 IGO. "],["introduction.html", "Chapter 1 Introduction 1.1 Importance of preserving soil legacy data. 1.2 Benefits of a centralised data repository for soil data.", " Chapter 1 Introduction Legacy soil data is regarded as a crucial scientific resource in assessing ecosystem health and biodiversity as well as in Digital Soil Mapping (DSM) in studies (FAO and WHO 2022). Historical soil data helps identify soil quality trends, guiding environmental cleanup and rehabilitation efforts. However, the use of this data faces challenges due to its inherent heterogeneity. This data is often fragmented, decentralised, in different formats, both paper or digital, and without any kind of harmonisation and quality assessment. This variability in legacy databases arises from multiple factors: the specific goals set for sampling campaigns, the level of precision in measurements and reported data, variations in the depth distribution and the maximum depth of soil samples, the geographical extent of the sampling, and the methodology employed in the sampling design (Arrouays et al. 2017; Schillaci et al. 2019; Dobos et al. 2010; Ramos et al. 2017). This diversity in the data underscores the complexity and challenges in utilising legacy soil data effectively. A Soil Legacy Data Repository is a storage space to deposit data sets associated with legacy soil data. It serves as a critical tool in both environmental and agricultural planning and provides the foundational data necessary for informed decision-making, sustainable management practices, and the development of strategies to address current and future challenges concerning soil management and conservation. In the realm of environmental planning, the role of a national soil legacy data repository is multifaceted and significant. For climate change adaptation and mitigation, soil data is instrumental in identifying areas with high carbon sequestration potential, thereby guiding efforts to mitigate climate change effects. This data also plays a vital role in assessing the impact of climate change on various ecosystems, helping in predictive analyses. When it comes to biodiversity conservation, soil data is key in pinpointing degraded areas, which is crucial for habitat restoration and conserving biodiversity. Additionally, it provides essential information for the conservation strategies of various flora and fauna, dependent on specific soil conditions. Historical soil data is invaluable in revealing areas with a history of contamination. This facilitates targeted clean-up and remediation efforts, and also aids in the long-term monitoring of pollution trends and the effectiveness of various pollution control measures. Regarding sustainable land use planning, soil data offers insights crucial for informed decision-making in urban development, including zoning, infrastructure development, and green space allocation. Moreover, understanding soil composition and structure is vital for designing effective erosion and flood control measures, ensuring sustainable and environmentally friendly land use. In agricultural planning, the use of a national soil legacy data repository is exceptionally valuable and diverse. Soil data plays a crucial role in crop management and planning, particularly in crop suitability analysis, where it helps determine the most suitable crops for specific areas, thereby optimising agricultural output. It also supports precision agriculture practices, enabling the targeted application of fertilisers and water, which enhances efficiency and productivity. Soil fertility management is another area where soil data proves to be indispensable. Historical data on soil nutrient levels assist in the development of efficient and environmentally friendly fertiliser application strategies. Moreover, the continuous monitoring of soil health, enabled by this data, guides interventions to maintain or improve soil fertility. In the context of disease and pest management, soil conditions have a notable influence on the prevalence of certain pests and diseases. Historical soil data enables predictive analysis to forecast and manage these challenges effectively. This data also aids in the development of integrated pest management strategies, which are less harmful to the environment and more sustainable in the long term. Water resource management greatly benefits from soil data as well. Information regarding soil water retention and drainage properties is crucial for efficient irrigation planning and water conservation strategies, which are increasingly important in the face of global water scarcity. Additionally, soil data is instrumental in identifying areas more susceptible to drought, guiding the implementation of drought-resilient agricultural practices. In risk assessment and management, soil data is necessary in assessing risks such as soil degradation, erosion, and salinization, all of which can significantly impact agricultural productivity. Historical soil data also plays a critical role in the development of climate-resilient farming practices, ensuring sustainable agricultural productivity under the evolving conditions brought about by climate change. The significance of soil data repositories is clear in the context of digital soil mapping, a field gaining increasing importance in agriculture, environmental management, and climate change mitigation. Data repositories make complex soil information more accessible, simplifying it for a wide range of users, and act as crucial sources of the data necessary for training digital mapping algorithms. This data, encompassing historical and current soil information, is key to calibrating digital soil models, thereby sharpening the accuracy and enhancing the resolution of soil maps. In this context, soil data repositories play an essential role in predictive modelling addressing key issues such as food security and global environmental changes. They aid in the projection of soil changes under various scenarios like climate change and land use shifts and enabling broad-scale soil assessments at regional and global levels. They assist in identifying risks such as soil erosion and contamination, which are vital for informed and sustainable land management practices. By merging soil data with other environmental datasets, these repositories enable interdisciplinary research, pushing the boundaries in soil science and the methodologies used in digital soil mapping. 1.1 Importance of preserving soil legacy data. Soil legacy data, which includes historical soil information gathered over the years, provides a unique insight into how soil conditions evolve over time. These datasets capture the changes in soil properties over decades, making it extremely valuable for understanding natural processes and the effects of human activities on soils. This is crucial in understanding how soil characteristics, such as soil organic content, nutrient status or pollution concentrations, evolve due to natural processes or human activities. Thus, it helps in analysing the impact of environmental changes, like climate change, deforestation and land-use changes on soil health, informs sustainable land use and provides a scientific basis for formulating regulations related to soil protection, ensuring balance between growth and environmental preservation. Legacy data can offer insights into aspects such as global food security. Soil legacy data can be used to determine historical trends in soils that can feed predictive models to anticipate changes in agricultural productivity and food security. Preserving soil legacy data is not just about maintaining records; itâ€™s about retaining a wealth of knowledge that holds the key to understanding our past, managing its present, and safeguarding its future. This data is a cornerstone for multidisciplinary research, policy-making, and sustainable development, impacting everything from local farming practices to global climate change strategies. 1.2 Benefits of a centralised data repository for soil data. A centralised data repository for soil data offers a multitude of benefits, spanning from enhanced accessibility and data sharing to improved data quality and informed decision-making. Accessibility: A single repository provides a unified access point for various stakeholders, including researchers, farmers, policymakers, and educators, facilitating easy retrieval of information. Centralization promotes interdisciplinary research and collaboration by making soil data accessible across different scientific and agricultural disciplines. Quality: A centralised data repository ensures that data from various sources is standardised in format, making it easier to compare and analyse and providing better control over the quality of soil data, as it can be vetted and validated through standardised protocols. Efficient Data Management: Centralised repositories provide organised storage, making it easier to manage vast amounts of soil data efficiently. Central repositories ensure long-term preservation of soil data, protecting it from loss due to localised issues like technical failures or organisational changes. Improved Data Analysis and Research: Having a centralised repository means researchers can access more comprehensive data sets, leading to more robust and inclusive research outcomes. Centralization facilitates the application of advanced data analytics, including AI and machine learning, to uncover deeper insights and patterns in soil data. Support for Policy and Decision Making: Access to comprehensive soil data aids policymakers in developing informed, evidence-based agricultural and environmental policies. This support can be crucial in managing risks related to agriculture, such as soil degradation, contamination, and climate change impacts. Enhanced Educational and Outreach Opportunities: A centralised soil data repository serves as an invaluable resource for educational institutions, enhancing learning and research opportunities for students.and it can also play a role in raising public awareness about soil health and sustainable agricultural practices. Facilitation of Digital Initiatives: Centralised data repositories are essential for digital soil mapping initiatives, providing the necessary data to create detailed and accurate soil maps. They facilitate the integration of soil data with other technological tools, like GIS and remote sensing, enhancing the scope of soil analysis and interpretation. Global Collaboration and Benchmarking: A centralised repository can serve as a platform for international collaboration, sharing best practices and data across borders. It allows for benchmarking and comparative studies at a global scale, essential for understanding and addressing global soil health issues. A centralised data repository for soil data is a powerful tool that can transform how soil information is managed and utilised. By providing a platform for standardised, high-quality, and accessible soil data, it supports a range of activities from scientific research to policy making, ultimately contributing to more sustainable and informed management of soil resources worldwide. This manual has been written in bookdown (Xie 2016). References "],["datacollection.html", "Chapter 2 Data Collection, Inclusion Criteria and Quality Control 2.1 Data Collection Guidelines 2.2 Data Inclusion Guidelines 2.3 Data Quality Guidelines", " Chapter 2 Data Collection, Inclusion Criteria and Quality Control One of the challenges with soil legacy data is its variability. The source data is often diverse, including different ranges of soil properties, formats and quality standards. This poses challenges in data integration, analysis, and interpretation. The methods used to collect this data can vary significantly, encompassing different sampling techniques, laboratory analyses, and documentation processes, affecting the comparability of the data. Legacy data might have been collected using less precise methods compared to modern techniques, leading to potential issues with accuracy and reliability. Furthermore, there can be gaps and limitations in the data, either spatially or temporally, due to historical constraints in data collection efforts, with certain areas or time periods potentially being under-represented. Despite these challenges, soil legacy data is often rich in contextual information, including land use history, climate data, and vegetation cover, adding invaluable context to soil studies, and it is crucial for establishing baseline conditions, against which current soil properties and changes can be assessed. The creation of a National Soil Legacy Data Repository encompasses both proper methods and processes to gather soil data - data collection, standards and requirements for data to be added to the repository - data inclusion, and for quality assessment. Establishing guidelines for the collection, inclusion and quality of soil legacy data is crucial for ensuring the dataâ€™s relevance, consistency and usefulness of the repository. There are several methods to retrieve information from legacy data. In the case that the original data is stored in the form of a digital database, its inclusion is direct even if some data harmonization is often required. However, in most of the cases, the original data is in paper format. In these cases the translation of the unstructured data in paper to a structured digital database can be done by: * Manual Data Entry: Operators read the documents and manually transpose the data into the system. This is highly time-consuming and it is prone to mistakes. The output data has to be doubled checked to ensure data quality and consistency. * Semi-automated text detection: Using OCR technology. * Use Artificial Intelligence and Machine Learning: Documents are analysed and Data is extracted using Artificial Intelligence. This method makes use of combinations of techniques such as OCR, Deep Learning, Natural Language Processing (NLP) (Hsu et al. 2022), etc to extract and classify the information from unstructured data and reduces considerably the processing time. There are application facilitating this methodology such as Google Cloud AI. 2.1 Data Collection Guidelines Source Identification: Collect data from reliable and verifiable sources, such as academic institutions, government agencies, and reputable research organisations. Ensure that the data collection methods and instruments used are documented and meet accepted scientific standards. Data Organization: Include information about data sources, land-use, geology and topography of the soil profile. Add an unique identifier to each soil profile in the database. Include the location coordinates and the EPSG code of the coordinate system of each soil profile. Include all soil horizons separately in the database. All horizons in the same soil profile must share the same profile ID. Include top and bottom depths of each horizon in the soil profile. Include all available soil physico-chemical properties such as colour, texture, sand, silk and clay contents, bulk density, pH, organic matter content, nutrient levels, etc. Include information about analytical methods used if available. Include the date or period of data collection for each dataset to determine the representative timeframe and to understand its historical context. Methodological Consistency: Where possible, use data collected using consistent methodologies to facilitate comparability. Document any methodological variations that could affect data interpretation. Ensure that documentation is provided whenever any of the values for the physical or chemical properties are estimated, such as when using pedotransfer functions. 2.2 Data Inclusion Guidelines Relevance: Data should be relevant to the specific objectives of the soil legacy data repository, such as agricultural planning, environmental monitoring, or climate change studies. Quality and Accuracy: Prioritise data that has been validated and where the accuracy is known or can be reasonably estimated. Include information about any known limitations or uncertainties in the data. Standardisation and Harmonization: Data should, as far as possible, conform to standard formats and units to facilitate integration and analysis. Conversion or normalisation procedures should be applied to data that is not in standard formats. Consider Soil Layer Harmonization when preparing data for DSM analyses. Completeness: Prefer datasets that are complete or near-complete for the variables and geographical areas they cover. Document any significant gaps in the data. Metadata: Ensure that each dataset includes comprehensive metadata detailing data collection methods, geographic location, time of collection, data collector, and any processing or analysis that has been performed. Legal and Ethical Considerations: Only include data that can be legally shared and used, respecting copyright and privacy concerns. Obtain necessary permissions and adhere to data sharing and usage policies. 2.3 Data Quality Guidelines Establishment of Accuracy Thresholds: Ensure that measured property values fall within feasible thresholds considering its soil type, land use, parent material and climatic conditions. Set acceptable ranges of variance for soil pH levels, nutrient content, and other measurable properties. When dealing with compositional data properties, such as the relative content of sand, silt and clay, ensure that the sum of the relative values is up to 100. Methodological Consistency: Ensure consistency in the methodologies used to collect and analyse soil data across different times and locations, to maintain data accuracy. Error Identification and Documentation: Implement processes for identifying and documenting potential errors or anomalies in the data. Develop protocols for handling data that falls outside the established accuracy thresholds, or values falling under the detection limit of the measurement device. Regular Data Quality Assessments: Conduct regular audits and quality assessments of the data to ensure ongoing accuracy. Update the repository with new findings or corrections as needed. Use of Statistical Methods for Validation: Apply statistical methods to assess and confirm the accuracy of the data. This can include techniques like regression analysis, error propagation, or uncertainty quantification. Metadata Requirements: Require comprehensive metadata for each dataset, detailing the accuracy assessments, error margins, and any relevant quality control measures. By following these guidelines, it can be ensured that the soil legacy data collected and included in the repository is of high quality, relevant, and useful for various applications. References "],["data_organization.html", "Chapter 3 Data Organization 3.1 Guidelines for Data Format 3.2 Guidelines for Data Structure", " Chapter 3 Data Organization Various international standards do exist to ensure the quality of soil data. However, they tend to focus on general principles and quality-assurance frameworks rather than the detail of describing data quality (Biggs and Searle 2017). In addition, data standards for soil data, generally, are not open and are difficult to find (E. 2021). This limits its implementation in real cases. Recently, there have been initiatives focused on incorporating ISO standards in the development and implementation of soil databases. These initiatives are in an early stage of documentation and their implementation are complicated and require a high degree of specialisation. Establishing guidelines for the data format and structure in a national soil legacy data repository is essential to ensure consistency, accessibility, and interoperability of the data. 3.1 Guidelines for Data Format Standardised File Formats: Adopt universally recognized and widely used file formats for data storage, such as CSV, JSON, or XML for tabular data, and GeoJSON or Shapefiles for geospatial data. Ensure these formats are compatible with common data analysis and GIS software. Consistency Across Datasets: Maintain consistent data formats across different datasets to facilitate easy integration and comparison. Implement standard naming conventions and data structures within these formats. Support for Metadata: Choose formats that support extensive metadata, allowing detailed documentation of data origin, collection methods, and any processing steps. Long-term Accessibility: Consider the longevity and future accessibility of data formats, favouring those that are less likely to become obsolete. 3.2 Guidelines for Data Structure Hierarchical Organization: Organise data in a clear, hierarchical structure, such as by geographic region, soil type, or collection date, to enhance user navigation and data retrieval. Uniformity in Data Representation: Standardise how different types of data are represented (e.g., consistent units of measurement, scales, and categorizations). Integration of Spatial Data: Structure the data to integrate seamlessly with spatial information, ensuring compatibility with GIS and mapping tools. Modular and Scalable Structure: Design the data structure to be modular and scalable, allowing for easy updates and the addition of new data without disrupting existing structures. Data Relationship Mapping: Clearly define and document the relationships between different datasets and variables within the repository. This can include relational database schemas or data dictionaries. Ease of Analysis: Structure data in a way that is conducive to typical analyses and queries, considering the needs of end-users such as researchers, policymakers, and farmers. Compliance with Standards: Adhere to relevant international standards, such as those from ISO or INSPIRE (Infrastructure for Spatial Information in Europe), for data structure and interoperability. References "],["data_accessibility.html", "Chapter 4 Data Accessibility and Sharing", " Chapter 4 Data Accessibility and Sharing Blablabla "],["integration_DSM.html", "Chapter 5 Integration with Digital Soil Mapping", " Chapter 5 Integration with Digital Soil Mapping As described previously, a proper Soil Legacy Data Repository constitutes a valuable resource of data in many fields of application. In particular, it is highly valuable in Digital Soil Mapping implementations to derive maps of soil properties from existing data (CarrÃ©, McBratney, and Minasny 2007). The integration of such data in DSM activities requires additional data harmonisation procedures to ensure that the estimations comply with a common set of rules: the dataset has to include the same properties in relation to the same soil depth interval, which generally is not the case in databases of soil data. Soil profiles are formed by soil horizons, with properties measured at the soil horizon depth, and harmonisation of soil profile layers, to determine the values of properties at the same soil depth, is required to derive a sound database to be used within a DSM framework. This requirement is addressed at Global Soil Partnership within Pillar 4 - enhance the quantity and quality of soil data and information: data collection, analysis, validation, reporting, monitoring and integration with other disciplines; and Pillar 5 - harmonisation of methods, measurements and indicators for the sustainable management and protection of soil resources. In this sense, methods for soil data layer harmonisation have been developed at GSP. The procedure is described in section 6.5 of the Global Soil Nutrient and Nutrient Budgets maps (GSNmap) Phase I Technical manual. (Harmonize Soil Layer Depths). This methodology makes use of splines interpolation methods to derive a harmonised database at fixed soil depths. A friendly integration of such analyses is available within a Shiny web application at the site https://geoforsk.shinyapps.io/Harmonize/ (Figure 5.1). You can download an example file to use within the application at http://www.geoforsk.com/datos/fao/example.xlsx. Figure 5.1: Output of the Shiny app to harmonize soil properties to fixed soil depths References "],["guidelines_1.html", "Chapter 6 Guidelines for Digitalization of Soil Profile Data", " Chapter 6 Guidelines for Digitalization of Soil Profile Data New Zealand National Soil Data Repository (NSDR): The National Soils Database (NSD) in New Zealand is a critical part of the soil data legacy. It includes descriptions and profile data collected from soil pits, with around 1500 New Zealand samples. These samples cover chemical, physical, and mineralogical characteristics of the soils. The NSDR hosts the original NSD and is designed as a versatile soil observation database, capable of generating new soil information from a diverse set of data sources. This includes soil quality data, National Soils database data, and DSM products. Australia Soil and Landscape Grids: In Australia, the Soil and Landscape Grids were produced based on legacy soil data compiled in the National Soil Site Collation database. This database meets the specifications of the GlobalSoilMap project and includes 13 soil attribute surfaces that are publicly available. The predictions in this database were performed using cubist-kriging, a method for soil data analysisâ€‹â€‹. United States Web Soil Survey (WSS): The Web Soil Survey (WSS) provides soil data and information produced by the National Cooperative Soil Survey. Operated by the USDA Natural Resources Conservation Service (NRCS), it offers access to one of the largest natural resource information systems in the world. The WSS is a public-facing tool that provides comprehensive soil data for various applicationsâ€‹â€‹. European Soil Data Centre (ESDAC): The European Soil Data Centre (ESDAC) serves as a significant repository for soil-related data. ESDAC stands as the thematic centre for soil data in Europe, aiming to be the single reference point for all relevant soil data and information at the European level. It provides access to various authoritative European soil-related datasets such as the European Soil Database (ESDB). Unlike national soil data repositories that focus on specific countries or regions, ESDAC offers a broader perspective, presenting detailed soil information on an European scale. "],["guidelines_2.html", "Chapter 7 Guidelines for Digitalization of Legacy Soil Maps 7.1 Project Planning and Scope Definition 7.2 Data Collection and Inventory 7.3 Preparation of Paper Maps for Digitization 7.4 Digitization Process 7.5 Georeferencing and Rectification 7.6 Data Extraction and Vectorization 7.7 Metadata Creation and Standardization 7.8 Database Development and Data Import 7.9 Quality Assurance and Data Validation 7.10 Accessibility and User Interface Design 7.11 Maintenance and Updating 7.12 Legal and Ethical Considerations 7.13 Outreach and User Training", " Chapter 7 Guidelines for Digitalization of Legacy Soil Maps Legacy soil maps are a valuable source of information about the distribution of soils and its properties. These maps are in the form of paper sheets that must be translated into proper digital formats. Building a digital repository for legacy maps is a detailed and methodical process that transforms valuable historical map data in paper format, into a digital format thatâ€™s accessible, accurate, and useful for various applications. Each step, from preparation and digitization to database development and user interface design, plays a critical role in creating a repository that meets the needs of its users and stands the test of time. Creating a digital repository for legacy maps involves a series of methodical steps. Below is a comprehensive guideline of to facilitate this process: 7.1 Project Planning and Scope Definition Objective Setting: Define the purpose and scope of the digital repository, including the type of maps and associated information to be digitized, as well as the structure of the database, file naming and structure of the data storage. Allocate essential resources such as skilled personnel, advanced technology, and sufficient funding. Digitizing large maps necessitates specialized staff and appropriate digitizing equipment. While small scanners can be used, large-format scanners are preferable for efficiency in the digitization process. Given that digitization generates substantial file sizes, adequate data storage solutions must be in place to handle the considerable volume of data produced. The digitization process is labour-intensive and demands significant time investment, necessitating careful financial planning to cover the costs associated with staffing and other related expenses. Timeline Establishment: Developing a comprehensive project timeline is crucial for the successful completion of the digital repository. This involves delineating specific milestones and setting firm deadlines for each phase of the project. Start by mapping out the initial stages, such as data collection, preparation of paper maps, and the procurement of necessary equipment and resources. Assign realistic timeframes to each of these activities, considering factors like the volume of maps to be digitized, the availability of resources, and the complexity of the tasks. Following the preparation phase, outline the time required for the actual digitization process. This includes scanning, quality checks, georeferencing, and rectification of the maps. Allocate sufficient time for data extraction, vectorization, and the subsequent integration of this data into the database, ensuring that each step is afforded the attention it requires for accuracy and thoroughness. Next, incorporate time for the creation and standardization of metadata, a critical component that facilitates data retrieval and utilization. Factor in additional time for database development and the importing of digitized maps and metadata. Include a phase for quality assurance and data validation to ensure the integrity and accuracy of the digital repository. This phase should allow for review, error correction, and cross-verification against original sources. Finally, designate time for discussing data accessibility, and the implementation of potential training programs for developers. Itâ€™s essential to also include ongoing maintenance and regular updates in the timeline. 7.2 Data Collection and Inventory Map Sourcing: Gather all paper maps that need to be digitized. Inventory Creation: Create an inventory of these maps, noting their condition, scale, date, and any other relevant details. Catalogue each map in detail, including metadata like geographical coverage, scale, date of creation, and source. 7.3 Preparation of Paper Maps for Digitization Map Cleaning and Repair: This step is important to enhance the quality of the scans, ensuring that the digitized versions accurately reflect the original maps. The goal is to prepare the maps in a way that maximizes the clarity and readability of the digital images, thereby facilitating a more precise and effective digitization process. Ensure maps are clean and repair any damages to improve scan quality. Special attention should be paid to preserving the integrity of the maps during cleaning and repair, using appropriate archival materials and techniques to avoid further deterioration. 7.4 Digitization Process Scanning: Choose appropriate scanning technology based on map size, quality, and detail. Carefully scan each map to create digital copies, ensuring high resolution and clarity. Scan the maps in the way that north points to the top. Quality Control: Conduct quality checks to ensure scans are clear and accurate representations of the original maps. Legacy maps are often stored in unfavourable ambient conditions. Take into consideration that characteristics such as room moisture can alter the size of the maps and cause deformations that can be a source of uncertainty in later steps of digitalization. 7.5 Georeferencing and Rectification Georeferencing: Assign geographic coordinates to the digitized maps so they align with real-world locations. Rectification: Correct any distortions in the maps to ensure geographical accuracy. Steps in QGIS. 1) Add a vector layer covering the extension of the map. It can be a shapefile with the national boundaries. Use a map with a known coordinate system - i.e.Â EPSG:4326 Create a grid of reference as the coordinate grid in the paper map. Vector -&gt; Research Tools -&gt; Create Grid Figure 7.1: Create Grid options Figure 7.2: Create Grid dialog box Open the Georeferencer: Layer -&gt; Georeferencer. The Georeferencer dialog box will appear. Figure 7.3: Georeferencer box 7.6 Data Extraction and Vectorization Extracting Information: Extract relevant information from the maps, including geographical features, legends, and notes. Vectorization: Convert map features into vector data for GIS compatibility and manipulation. 7.7 Metadata Creation and Standardization Metadata Documentation: Create detailed metadata for each map, including information on original source, date, scale, georeferencing details, and data accuracy. Standardization: Ensure metadata is standardized across all maps for consistency. 7.8 Database Development and Data Import Database Design: Develop a database structure that accommodates maps and associated metadata. Data Import: Import digitized maps and metadata into the database. 7.9 Quality Assurance and Data Validation Error Checking: Perform thorough checks to identify and correct any errors or inconsistencies in the data. Validation: Validate the data against original maps and other reliable sources for accuracy. 7.10 Accessibility and User Interface Design Access Permissions: Determine access levels for different users. User Interface: Develop a user-friendly interface for the digital repository, allowing easy search, access, and navigation. 7.11 Maintenance and Updating Regular Updates: Establish procedures for regularly updating the repository with new data or corrections. Maintenance Plan: Develop a maintenance plan for the digital repository, including regular backups and technical support. 7.12 Legal and Ethical Considerations Copyright Compliance: Ensure digitization and sharing of map data comply with copyright laws. Ethical Standards: Adhere to ethical standards in data sharing and usage. 7.13 Outreach and User Training Promotion: Promote the digital repository among potential users. Training Programs: Offer training programs for users to effectively utilize the repository. "],["automated-techniques-for-data-extraction..html", "Chapter 8 Automated techniques for data extraction.", " Chapter 8 Automated techniques for data extraction. Segmentation library(reticulate) img = mpimg.imread(&#39;/content/example_map.jpeg&#39;) imgplot = plt.imshow(img) plt.show() # Build an array of labels for training the segmentation. # Here we use rectangles but visualization libraries such as plotly # (and napari?) can be used to draw a mask on the image. training_labels = np.zeros(img.shape[:2], dtype=np.uint8) training_labels[6:162, 616:790] = 1 training_labels[5:76, 1638:1898] = 1 training_labels[5:190, 13:500] = 1 training_labels[122:236, 911:969] = 2 training_labels[757:974, 1581:1746] = 2 training_labels[843:990, 381:534] = 2 training_labels[281:304, 39:150] = 3 training_labels[982:1004, 1347:1399] = 3 training_labels[1002:1037, 1667:1703] = 3 training_labels[358:385, 902:927] = 4 training_labels[563:568, 802:863] = 4 training_labels[347:369, 1119:1155] = 4 training_labels[293:306, 1034:1087] = 5 training_labels[422:449, 833:876] = 5 training_labels[600:622, 653:701] = 5 plt.imshow(img) plt.contour(training_labels) plt.title(&quot;Image and segmentation boundaries&quot;) sigma_min = 1 sigma_max = 16 features_func = partial(feature.multiscale_basic_features, intensity=True, edges=False, texture=True, sigma_min=sigma_min, sigma_max=sigma_max, channel_axis=-1) features = features_func(img) clf = RandomForestClassifier(n_estimators=50, n_jobs=-1, max_depth=10, max_samples=0.05) clf = future.fit_segmenter(training_labels, features, clf) result = future.predict_segmenter(features, clf) fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(9, 4)) ax[0].imshow(segmentation.mark_boundaries(img, result, mode=&#39;thick&#39;)) ax[0].contour(training_labels) ax[0].set_title(&#39;Image, mask and segmentation boundaries&#39;) ax[1].imshow(result) ax[1].set_title(&#39;Segmentation&#39;) fig.tight_layout() fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(9, 4)) ax[0].imshow(segmentation.mark_boundaries(img, result, mode=&#39;thick&#39;)) ax[0].contour(training_labels) ax[0].set_title(&#39;Image, mask and segmentation boundaries&#39;) ax[1].imshow(result) ax[1].set_title(&#39;Segmentation&#39;) fig.tight_layout() im = Image.fromarray(result) im.save(&quot;/content/result.jpeg&quot;) Predict new images img_test = mpimg.imread(&#39;/content/testing_map.jpg&#39;) features_new = features_func(img_test) result_new = future.predict_segmenter(features_new, clf) fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(6, 4)) ax[0].imshow(segmentation.mark_boundaries(img_test, result_new, mode=&#39;thick&#39;)) ax[0].set_title(&#39;Image&#39;) ax[1].imshow(result_new) ax[1].set_title(&#39;Segmentation&#39;) fig.tight_layout() plt.show() Save predictions to disk im = Image.fromarray(result_new) im.save(&quot;/content/result_test.jpeg&quot;) "],["appendix-i.html", "Appendix I", " Appendix I SOME NOTABLE NATIONAL SOIL LEGACY DATA REPOSITORIES New Zealand National Soil Data Repository (NSDR): The National Soils Database (NSD) in New Zealand is a critical part of the soil data legacy. It includes descriptions and profile data collected from soil pits, with around 1500 New Zealand samples. These samples cover chemical, physical, and mineralogical characteristics of the soils. The NSDR hosts the original NSD and is designed as a versatile soil observation database, capable of generating new soil information from a diverse set of data sources. This includes soil quality data, National Soils database data, and DSM products. Australia Soil and Landscape Grids: In Australia, the Soil and Landscape Grids were produced based on legacy soil data compiled in the National Soil Site Collation database. This database meets the specifications of the GlobalSoilMap project and includes 13 soil attribute surfaces that are publicly available. The predictions in this database were performed using cubist-kriging, a method for soil data analysis. United States Web Soil Survey (WSS): The Web Soil Survey (WSS) provides soil data and information produced by the National Cooperative Soil Survey. Operated by the USDA Natural Resources Conservation Service (NRCS), it offers access to one of the largest natural resource information systems in the world. The WSS is a public-facing tool that provides comprehensive soil data for various applicationsâ€‹â€‹. European Soil Data Centre (ESDAC): The European Soil Data Centre (ESDAC) serves as a significant repository for soil-related data. ESDAC stands as the thematic centre for soil data in Europe, aiming to be the single reference point for all relevant soil data and information at the European level. It provides access to various authoritative European soil-related datasets such as the European Soil Database (ESDB). Unlike national soil data repositories that focus on specific countries or regions, ESDAC offers a broader perspective, presenting detailed soil information on an European scale. "],["appendix-ii.html", "Appendix II", " Appendix II ISO STANDARDS RELATED TO SOIL QUALITY AND GEOGRAPHIC INFORMATION SERVICES The development of a soil legacy data repository can be guided by various International Organisation for Standardisation (ISO) standards related to soil quality, ensuring the accuracy, consistency, and international compatibility of the data within soil projects. Some key ISO standards relevant to this field include: ISO TC-190 -ISO TC-345: soil properties. ISO 11074 &amp; ISO 11074/Amd1:2020: This standard focuses on Soil Quality Vocabulary, providing a comprehensive set of terms and definitions relevant to soil quality, essential for standardising communication and understanding in soil data repositories. ISO 15903: This standard focuses on Format and recording of soil and site information. ISO 25177: Field Soil Description. It provides guidelines for describing soil in the field, which is crucial for accurate data collection and categorization in a repository. ISO 18400 Series: These standards cover various aspects of Soil Quality Sampling, including: ISO 18400-101: Framework for preparation and application of a sampling plan. ISO 18400-102: Selection and application of sampling techniques. ISO 18400-103: Safety considerations during soil sampling. ISO 18400-104: Strategies for soil sampling. ISO 18400-106: Quality control and assurance in soil sampling. ISO 18400-107: Recording and reporting of soil sampling data. ISO 18400-203: Investigation of potentially contaminated sites. ISO 18400-205: Guidance on the procedure for investigating natural, near-natural, and cultivated sites. ISO 28258: This standard, Soil Quality â€” Digital Exchange of Soil-Related Data, provides a framework for the digital exchange of soil data, including a detailed domain model for soil observations. It is based on the Observations and Measurements (O&amp;M) standard issued by the OGC and ISO, and is particularly relevant for the structuring and digital handling of soil data in repositories. Data model for the ISO 28258 domain model: Postgres data model implementing the ISO-28258 standard, at ISRIC; Github at ISO 23992 &amp; ISO 23992/Amd1:2019: Soil Quality â€” Framework for detailed recording and monitoring of changes in dynamic soil properties. These standards offer comprehensive guidelines for various aspects of soil data management, from vocabulary and field description to sampling techniques, safety, reporting, and digital data exchange. Implementing these standards in the development of a soil legacy data repository ensures that the data is accurate, consistent, and aligned with international best practices. ISO 19168-1: Geographic information â€” Geospatial API for features â€” Part 1: Core. This document specifies the behaviour of Web APIs that provide access to features in a dataset in a manner independent of the underlying data store. This document defines discovery and query operations. ISO 19115-1: Geographic information â€“ Metadata. An internationally-adopted schema for describing geographic information and services. It provides information about the identification, the extent, the quality, the spatial and temporal schema, spatial reference, and distribution of digital geographic data. ISO 19119: Geographic information services. Defines requirements for how platform neutral and platform specific specification of services shall be created, in order to allow for one service to be specified independently of one or more underlying distributed computing platforms. ISO/TS 19139-1: Defines XML-based encoding rules for conceptual schemas specifying types that describe geographic resources. The encoding rules support the UML profile as used in the UML models commonly used in the standards developed by ISO/TC 211. The encoding rules use XML schema for the output data structure schema. ISO 28258 domain model: Soil organic carbon content - ISO10694 (dry combustion method). pH - ISO TC 190 (1:5 suspension of soil in water or using the CaCl2 solution) Cation Exchange Capacity - ISO11260 (Barium Chloride) "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
